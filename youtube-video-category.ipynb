{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport re\nimport os\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import f1_score,accuracy_score,confusion_matrix,precision_score,f1_score,recall_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom nltk.stem import PorterStemmer\nimport nltk\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):   #print all the files in the input of kaggle\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-20T17:27:19.728795Z","iopub.execute_input":"2021-09-20T17:27:19.729155Z","iopub.status.idle":"2021-09-20T17:27:19.740502Z","shell.execute_reply.started":"2021-09-20T17:27:19.729124Z","shell.execute_reply":"2021-09-20T17:27:19.739467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url_train = \"https://raw.githubusercontent.com/ezhil384/Youtube-text-classification/master/train.csv\"\nurl_test = \"https://raw.githubusercontent.com/ezhil384/Youtube-text-classification/master/test.csv\"\ndownload1 = requests.get(url_train).content\ndownload2 = requests.get(url_test).content\ntrain_data = pd.read_csv(io.StringIO(download1.decode('utf-8')))\ntest_data = pd.read_csv(io.StringIO(download2.decode('utf-8')))\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T17:30:43.158527Z","iopub.execute_input":"2021-09-20T17:30:43.158917Z","iopub.status.idle":"2021-09-20T17:31:03.230559Z","shell.execute_reply.started":"2021-09-20T17:30:43.158883Z","shell.execute_reply":"2021-09-20T17:31:03.227455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/youtube/train.csv\")\ntest_data = pd.read_csv(\"../input/youtube/test.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T17:31:37.919141Z","iopub.execute_input":"2021-09-20T17:31:37.919524Z","iopub.status.idle":"2021-09-20T17:31:38.172579Z","shell.execute_reply.started":"2021-09-20T17:31:37.919488Z","shell.execute_reply":"2021-09-20T17:31:38.171586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:26:44.882519Z","iopub.execute_input":"2021-09-20T14:26:44.882928Z","iopub.status.idle":"2021-09-20T14:26:44.895225Z","shell.execute_reply.started":"2021-09-20T14:26:44.882894Z","shell.execute_reply":"2021-09-20T14:26:44.894036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)\nprint(train_data.isnull().values.any())   #check for dropping and pre processing\nprint(test_data.isnull().values.any())","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:26:47.501326Z","iopub.execute_input":"2021-09-20T14:26:47.501693Z","iopub.status.idle":"2021-09-20T14:26:47.511388Z","shell.execute_reply.started":"2021-09-20T14:26:47.501663Z","shell.execute_reply":"2021-09-20T14:26:47.51029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading stop words from nltk library\nending_words = set(stopwords.words('english'))\n\ndef desc_process_train(text, index, column):\n    if type(text) is not int:\n        s = \"\"\n        url = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?' #removing URL's\n        text = re.sub(url,' ',text)\n        email =r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+'  #removing mails\n        text = re.sub(email,' ', text)\n        text = re.sub('[^a-zA-Z0-9\\n]',' ',text)    #removing all special characters\n        text = re.sub('\\s+',' ',text)    #remove multi spaces\n        text = text.lower()\n        \n        for word in text.split():\n            if not word in ending_words:\n                s = s + word + \" \"\n        \n        train_data[column][index] = s\nfor ind, row in train_data.iterrows():\n    if type(row['description']) is str:\n        desc_process_train(row['description'], ind, 'description')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:26:51.628878Z","iopub.execute_input":"2021-09-20T14:26:51.629273Z","iopub.status.idle":"2021-09-20T14:26:59.464254Z","shell.execute_reply.started":"2021-09-20T14:26:51.62924Z","shell.execute_reply":"2021-09-20T14:26:59.463287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:27:06.891282Z","iopub.execute_input":"2021-09-20T14:27:06.89168Z","iopub.status.idle":"2021-09-20T14:27:06.905779Z","shell.execute_reply.started":"2021-09-20T14:27:06.891646Z","shell.execute_reply":"2021-09-20T14:27:06.904499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories=train_data['category_id'].unique()\nprint(categories)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:27:17.034418Z","iopub.execute_input":"2021-09-20T14:27:17.034806Z","iopub.status.idle":"2021-09-20T14:27:17.043463Z","shell.execute_reply.started":"2021-09-20T14:27:17.034765Z","shell.execute_reply":"2021-09-20T14:27:17.042217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data into train test and validate","metadata":{}},{"cell_type":"code","source":"y = train_data['category_id'].values\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\ntrain, test, y_train, y_test = train_test_split(train_data, y, stratify=y, test_size=0.2)\nprint('Data points in training data:', train.shape[0])\nprint('Data points in test data:', test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:27:20.986314Z","iopub.execute_input":"2021-09-20T14:27:20.986691Z","iopub.status.idle":"2021-09-20T14:27:21.00053Z","shell.execute_reply.started":"2021-09-20T14:27:20.986658Z","shell.execute_reply":"2021-09-20T14:27:20.999693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Graph to show the distribution of the data into all 16 categories","metadata":{}},{"cell_type":"code","source":"X_category=train['category_id'].value_counts()   #values in each category in X\nplt.figure(figsize=(12,12))\nsns.barplot(X_category.index, X_category.values, alpha=0.8)\nplt.title('Distribution of the categories in the training data set')\nplt.ylabel('No. of data')\nplt.xlabel('Category')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:27:49.998168Z","iopub.execute_input":"2021-09-20T14:27:49.998592Z","iopub.status.idle":"2021-09-20T14:27:50.360324Z","shell.execute_reply.started":"2021-09-20T14:27:49.998543Z","shell.execute_reply":"2021-09-20T14:27:50.359138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using tf_idf to vectorize the words present in the description","metadata":{}},{"cell_type":"code","source":"x_train=train['description']\nx_test=test['description']\nvector_texts = TfidfVectorizer(ngram_range=(1,2))\nx_tr_tfidf = vector_texts.fit_transform(x_train)\nx_test_tfidf = vector_texts.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:28:35.833019Z","iopub.execute_input":"2021-09-20T14:28:35.833546Z","iopub.status.idle":"2021-09-20T14:28:37.570508Z","shell.execute_reply.started":"2021-09-20T14:28:35.833514Z","shell.execute_reply":"2021-09-20T14:28:37.569444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Stochastic gradient descent classifier for a linear SVM to train the data and predict the results","metadata":{}},{"cell_type":"code","source":"clf = SGDClassifier(loss = 'hinge', alpha =0.00005, penalty='l2', class_weight='balanced', learning_rate='optimal',eta0=0.001, n_jobs = -1) \nclf.fit(x_tr_tfidf,y_train)\ny_pred = clf.predict(x_test_tfidf)\ny2_pred = clf.predict(x_tr_tfidf)\nprint(\"Accuracy on train set: %0.3f%%\"%(accuracy_score(y_train, y2_pred)*100))\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(15),range(15))\nsns.set(font_scale=1.4)#for label size\nlabels = categories\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted categories')\nplt.ylabel('Original categories')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T11:16:47.17394Z","iopub.execute_input":"2021-09-04T11:16:47.174255Z","iopub.status.idle":"2021-09-04T11:16:48.458606Z","shell.execute_reply.started":"2021-09-04T11:16:47.174229Z","shell.execute_reply":"2021-09-04T11:16:48.457723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using the API for SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nclvmf = SVC(kernel='linear',C=1.5,gamma='scale') \nclvmf.fit(x_tr_tfidf,y_train)\ny_predf = clvmf.predict(x_test_tfidf)\ny2_predf = clvmf.predict(x_tr_tfidf)\nprint(\"Accuracy on train set: %0.3f%%\"%(accuracy_score(y_train, y2_predf)*100))\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_predf)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_predf,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_predf,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_predf,average='macro')))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_predf), range(15),range(15))\nsns.set(font_scale=1.4)#for label size\nlabels = categories\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T14:28:59.262705Z","iopub.execute_input":"2021-09-20T14:28:59.263116Z","iopub.status.idle":"2021-09-20T14:29:34.37182Z","shell.execute_reply.started":"2021-09-20T14:28:59.263083Z","shell.execute_reply":"2021-09-20T14:29:34.370718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using XGBoost algorithm to check as an alternative if it performs well","metadata":{}},{"cell_type":"code","source":"xgb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3, random_state=0)\nxgb.fit(x_tr_tfidf,y_train)\ny_pred = xgb.predict(x_test_tfidf)\nprint(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\nprint(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred,average='macro')))\nprint(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred,average='macro')))\nprint(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred,average='macro')))\n\nprint(\"-\"*20, \"confusion matrix\", \"-\"*20)\nplt.figure(figsize=(12,8))\ndf_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(15),range(15))\nsns.set(font_scale=1.4)#for label size\nlabels = categories\nsns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g',xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Original Class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T09:45:35.3561Z","iopub.execute_input":"2021-09-04T09:45:35.356449Z","iopub.status.idle":"2021-09-04T09:49:53.935423Z","shell.execute_reply.started":"2021-09-04T09:45:35.356417Z","shell.execute_reply":"2021-09-04T09:49:53.934587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The test file data","metadata":{}},{"cell_type":"code","source":"def desc_process_test(text, index, column):\n    if type(text) is not int:\n        s = \"\"\n        url = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?' #removing URL's\n        text = re.sub(url,' ',text)\n        email =r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+'  #removing mails\n        text = re.sub(email,' ', text)\n        text = re.sub('[^a-zA-Z0-9\\n]',' ',text)    #removing all special characters\n        text = re.sub('\\s+',' ',text)    #remove multi spaces\n        text = text.lower()\n        \n        for word in text.split():\n            if not word in ending_words:\n                s = s + word + \" \"\n        \n        test_data[column][index] = s","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:01:45.211951Z","iopub.execute_input":"2021-09-20T15:01:45.212365Z","iopub.status.idle":"2021-09-20T15:01:45.220428Z","shell.execute_reply.started":"2021-09-20T15:01:45.212332Z","shell.execute_reply":"2021-09-20T15:01:45.219031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pre processing the description of the test dataset**","metadata":{}},{"cell_type":"code","source":"for index, row in test_data.iterrows():\n    if type(row['description']) is str:\n        desc_process_test(row['description'], index, 'description')","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:01:51.374995Z","iopub.execute_input":"2021-09-20T15:01:51.375382Z","iopub.status.idle":"2021-09-20T15:01:53.791348Z","shell.execute_reply.started":"2021-09-20T15:01:51.37535Z","shell.execute_reply":"2021-09-20T15:01:53.79032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:01:59.271281Z","iopub.execute_input":"2021-09-20T15:01:59.271673Z","iopub.status.idle":"2021-09-20T15:01:59.28287Z","shell.execute_reply.started":"2021-09-20T15:01:59.271639Z","shell.execute_reply":"2021-09-20T15:01:59.281802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorizing the test_dataset description word counts and then running the SVM classifier","metadata":{}},{"cell_type":"code","source":"x_testdata = vector_texts.transform(test_data['description'])\ny_testpred = clvmf.predict(x_testdata)\nprint(y_testpred)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:02:03.838836Z","iopub.execute_input":"2021-09-20T15:02:03.839218Z","iopub.status.idle":"2021-09-20T15:02:09.358654Z","shell.execute_reply.started":"2021-09-20T15:02:03.839162Z","shell.execute_reply":"2021-09-20T15:02:09.357363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_testpred = clf.predict(x_testdata)\nprint(y_testpred)","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:02:37.310595Z","iopub.execute_input":"2021-09-20T15:02:37.311123Z","iopub.status.idle":"2021-09-20T15:02:37.340043Z","shell.execute_reply.started":"2021-09-20T15:02:37.311076Z","shell.execute_reply":"2021-09-20T15:02:37.338584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the final csv file with the predictions","metadata":{}},{"cell_type":"code","source":"import csv\nmet = \"../input/youtube/test.csv\"\nwith open('submission.csv','w') as fappend:\n    csvwriter=csv.writer(fappend)\n    csvwriter.writerow(['video_id','category_id'])\nwith open(met, 'r') as fread,open('submission.csv','a') as fappend:\n    csvreader = csv.reader(fread)\n    csvwriter = csv.writer(fappend)\n    i=0\n    fr=0   #row 0 check to avoid first row with heading\n    for row in csvreader:\n        if fr==0:\n            fr=1\n            continue\n        csvwriter.writerow([row[0],y_testpred[i]])\n        i=i+1\n    \n          ","metadata":{"execution":{"iopub.status.busy":"2021-09-20T15:02:40.128569Z","iopub.execute_input":"2021-09-20T15:02:40.128918Z","iopub.status.idle":"2021-09-20T15:02:40.221635Z","shell.execute_reply.started":"2021-09-20T15:02:40.128887Z","shell.execute_reply":"2021-09-20T15:02:40.22061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# By this we conclude that linear SVM is the best model for text classification for the 16 categories.","metadata":{}}]}